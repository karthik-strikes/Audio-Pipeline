{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik-strikes/Audio-Pipeline/blob/main/Whisper_Transcription_Timestamp\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCmjcOc9yEtQ"
      },
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn1c-CoDv2kw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a552822-cece-456d-ffb8-fd7dc67013f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/m-bain/whisperX.git@78dcfaab51005aa703ee21375f81ed31bc248560\n",
            "  Cloning https://github.com/m-bain/whisperX.git (to revision 78dcfaab51005aa703ee21375f81ed31bc248560) to /tmp/pip-req-build-9k9huw7b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperX.git /tmp/pip-req-build-9k9huw7b\n",
            "  Running command git rev-parse -q --verify 'sha^78dcfaab51005aa703ee21375f81ed31bc248560'\n",
            "  Running command git fetch -q https://github.com/m-bain/whisperX.git 78dcfaab51005aa703ee21375f81ed31bc248560\n",
            "  Running command git checkout -q 78dcfaab51005aa703ee21375f81ed31bc248560\n",
            "  Resolved https://github.com/m-bain/whisperX.git to commit 78dcfaab51005aa703ee21375f81ed31bc248560\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchaudio>=2 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (2.4.1+cu121)\n",
            "Collecting faster-whisper==1.0.0 (from whisperx==3.1.1)\n",
            "  Downloading faster_whisper-1.0.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (4.44.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (2.2.2)\n",
            "Requirement already satisfied: setuptools>=65 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (71.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (3.8.1)\n",
            "Collecting pyannote.audio==3.1.1 (from whisperx==3.1.1)\n",
            "  Downloading pyannote.audio-3.1.1-py2.py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting av==11.* (from faster-whisper==1.0.0->whisperx==3.1.1)\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster-whisper==1.0.0->whisperx==3.1.1)\n",
            "  Downloading ctranslate2-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (0.24.7)\n",
            "Collecting tokenizers<0.16,>=0.13 (from faster-whisper==1.0.0->whisperx==3.1.1)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper==1.0.0->whisperx==3.1.1)\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting asteroid-filterbanks>=0.4 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.8.0)\n",
            "Collecting lightning>=2.0.1 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting omegaconf<3.0,>=2.1 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pyannote.core>=5.0.0 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.database>=5.0.1 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading pyannote.database-5.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pyannote.metrics>=3.2 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
            "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading pytorch_metric_learning-2.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (13.9.2)\n",
            "Collecting semver>=3.0.0 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.12.1)\n",
            "Collecting speechbrain>=0.5.14 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading speechbrain-1.0.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tensorboardX>=2.6 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting torch-audiomentations>=0.11.0 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading torch_audiomentations-0.11.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting torchmetrics>=0.11.0 (from pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (2024.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (2024.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (0.4.5)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers (from whisperx==3.1.1)\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<3.0,>=2.1->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (3.20.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.13.1)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (0.12.5)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (1.5.2)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (3.7.1)\n",
            "Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->whisperx==3.1.1) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (2.18.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.17.1)\n",
            "Collecting hyperpyyaml (from speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1) (0.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->whisperx==3.1.1) (1.3.0)\n",
            "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.10.2.post1)\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->whisperx==3.1.1) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (2024.8.30)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (3.10.10)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.0.8)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (3.1.4)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.0.35)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (3.5.0)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.5.4)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (4.0.3)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (4.3.6)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (3.1.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (0.2.0)\n",
            "Downloading faster_whisper-1.0.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.audio-3.1.1-py2.py3-none-any.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Downloading ctranslate2-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.2/37.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.database-5.1.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading pytorch_metric_learning-2.6.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading speechbrain-1.0.1-py3-none-any.whl (807 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.2/807.2 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_audiomentations-0.11.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.4.3-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: whisperx, antlr4-python3-runtime, docopt, julius\n",
            "  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisperx: filename=whisperx-3.1.1-py3-none-any.whl size=38607 sha256=4de1445078be18b180b12b38cc163ec65f05f4fba6a3ed0ed24aa5c6ab52c203\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/d3/58/f5849c3e001ad6c42f42a1ca4fb67310016a235133d7e081ed\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=7dcaca89cd690a0a42e068aa8d7da74558b326cab4930528a5287354db4fa89f\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=9657463ff2663a30c3d78eeb6a4c05d28e6dd186956c43c7c84ce009728ed115\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21869 sha256=e7dfbe25825489371206668f359c3c75b07571a29c01b758fd2eccaef37e6c96\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\n",
            "Successfully built whisperx antlr4-python3-runtime docopt julius\n",
            "Installing collected packages: primePy, docopt, antlr4-python3-runtime, tensorboardX, semver, ruamel.yaml.clib, omegaconf, Mako, lightning-utilities, humanfriendly, ctranslate2, colorlog, av, ruamel.yaml, pyannote.core, coloredlogs, alembic, torchmetrics, tokenizers, pytorch-metric-learning, optuna, onnxruntime, julius, hyperpyyaml, asteroid-filterbanks, transformers, torch-pitch-shift, speechbrain, pyannote.database, faster-whisper, torch-audiomentations, pytorch-lightning, pyannote.pipeline, pyannote.metrics, lightning, pyannote.audio, whisperx\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 av-11.0.0 coloredlogs-15.0.1 colorlog-6.8.2 ctranslate2-4.4.0 docopt-0.6.2 faster-whisper-1.0.0 humanfriendly-10.0 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.4.0 lightning-utilities-0.11.7 omegaconf-2.3.0 onnxruntime-1.19.2 optuna-4.0.0 primePy-1.3 pyannote.audio-3.1.1 pyannote.core-5.0.0 pyannote.database-5.1.0 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.4.0 pytorch-metric-learning-2.6.0 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 semver-3.0.2 speechbrain-1.0.1 tensorboardX-2.6.2.2 tokenizers-0.15.2 torch-audiomentations-0.11.1 torch-pitch-shift-1.2.5 torchmetrics-1.4.3 transformers-4.39.3 whisperx-3.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "9191bf46ed944e45a6f972ce9435819a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nemo-toolkit>=2.dev (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading nemo_toolkit-2.0.0rc1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fiddle (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading fiddle-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (0.24.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (1.26.4)\n",
            "Collecting onnx>=1.7.0 (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (2.8.2)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (0.18.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (71.0.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (2.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (4.66.5)\n",
            "Collecting wget (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (1.16.0)\n",
            "Collecting braceexpand (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (0.8.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (0.8.0)\n",
            "Collecting g2p-en (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting jiwer (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting kaldi-python-io (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kaldiio (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading kaldiio-2.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lhotse>=1.24.2 (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading lhotse-1.27.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (0.10.2.post1)\n",
            "Collecting marshmallow (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (24.1)\n",
            "Requirement already satisfied: pyannote.core in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (5.0.0)\n",
            "Requirement already satisfied: pyannote.metrics in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (3.2.1)\n",
            "Collecting pydub (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyloudnorm (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting resampy (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (1.13.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (0.12.1)\n",
            "Collecting sox (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading sox-1.5.0.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting texterrors (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading texterrors-0.5.1.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (2.2.1)\n",
            "Collecting hydra-core<=1.3.2,>1.3 (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf<=2.3 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (2.3.0)\n",
            "Requirement already satisfied: pytorch-lightning>2.2.1 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (2.4.0)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (1.4.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (4.39.3)\n",
            "Collecting wandb (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting webdataset>=0.2.86 (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading webdataset-0.2.100-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting datasets (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (7.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (2.2.2)\n",
            "Collecting sacremoses>=0.0.43 (from nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.10/dist-packages (from nemo-toolkit[asr]>=2.dev) (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (4.12.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core<=1.3.2,>1.3->nemo-toolkit[asr]>=2.dev) (4.9.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from lhotse>=1.24.2->nemo-toolkit[asr]>=2.dev) (3.0.1)\n",
            "Requirement already satisfied: click>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from lhotse>=1.24.2->nemo-toolkit[asr]>=2.dev) (8.1.7)\n",
            "Collecting cytoolz>=0.10.1 (from lhotse>=1.24.2->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading cytoolz-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting intervaltree>=3.1.0 (from lhotse>=1.24.2->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from lhotse>=1.24.2->nemo-toolkit[asr]>=2.dev) (0.9.0)\n",
            "Collecting lilcom>=1.1.0 (from lhotse>=1.24.2->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading lilcom-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo-toolkit[asr]>=2.dev) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo-toolkit[asr]>=2.dev) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo-toolkit[asr]>=2.dev) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo-toolkit[asr]>=2.dev) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo-toolkit[asr]>=2.dev) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->nemo-toolkit[asr]>=2.dev) (1.0.8)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (0.43.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.7.0->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.20.3)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>2.2.1->nemo-toolkit[asr]>=2.dev) (0.11.7)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses>=0.0.43->nemo-toolkit[asr]>=2.dev) (2024.9.11)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->nemo-toolkit[asr]>=2.dev) (1.17.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.1.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->nemo-toolkit[asr]>=2.dev) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->nemo-toolkit[asr]>=2.dev) (3.10.10)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from fiddle->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (1.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from fiddle->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (0.20.3)\n",
            "Collecting libcst (from fiddle->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading libcst-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p-en->nemo-toolkit[asr]>=2.dev) (3.8.1)\n",
            "Collecting distance>=0.1.3 (from g2p-en->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect->nemo-toolkit[asr]>=2.dev) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect->nemo-toolkit[asr]>=2.dev) (4.3.0)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nemo-toolkit[asr]>=2.dev) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->nemo-toolkit[asr]>=2.dev) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (1.16.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core->nemo-toolkit[asr]>=2.dev) (2.4.0)\n",
            "Requirement already satisfied: pyannote.database>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics->nemo-toolkit[asr]>=2.dev) (5.1.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics->nemo-toolkit[asr]>=2.dev) (0.6.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics->nemo-toolkit[asr]>=2.dev) (3.7.1)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from pyloudnorm->nemo-toolkit[asr]>=2.dev) (1.0.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (0.2.8)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.0.4)\n",
            "Collecting pybind11 (from texterrors->nemo-toolkit[asr]>=2.dev)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting plac (from texterrors->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading plac-1.4.3-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting loguru (from texterrors->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from texterrors->nemo-toolkit[asr]>=2.dev) (2.5.0)\n",
            "Collecting Levenshtein (from texterrors->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->nemo-toolkit[asr]>=2.dev) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->nemo-toolkit[asr]>=2.dev) (0.4.5)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->nemo-toolkit[asr]>=2.dev) (4.3.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo-toolkit[asr]>=2.dev) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading sentry_sdk-2.16.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting setproctitle (from wandb->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->nemo-toolkit[asr]>=2.dev) (2.22)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->lhotse>=1.24.2->nemo-toolkit[asr]>=2.dev) (0.12.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo-toolkit[asr]>=2.dev) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo-toolkit[asr]>=2.dev) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo-toolkit[asr]>=2.dev) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo-toolkit[asr]>=2.dev) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo-toolkit[asr]>=2.dev) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo-toolkit[asr]>=2.dev) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->nemo-toolkit[asr]>=2.dev) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (3.1.4)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (0.12.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->nemo-toolkit>=2.dev->nemo-toolkit[asr]>=2.dev) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo-toolkit[asr]>=2.dev)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (13.9.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->nemo-toolkit[asr]>=2.dev) (0.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo-toolkit[asr]>=2.dev) (0.1.2)\n",
            "Downloading nemo_toolkit-2.0.0rc1-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lhotse-1.27.0-py3-none-any.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.9/806.9 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdataset-0.2.100-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiddle-0.3.0-py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
            "Downloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lilcom-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.16.0-py2.py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.8/313.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libcst-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: kaldi-python-io, sox, texterrors, wget, distance, intervaltree\n",
            "  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=8952 sha256=31ba467e10ff2cc24aad2f57588e640a5231fcaaf444d96b89ff3eaf4f7c5b3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/23/5f/49d3a826be576faf61d84e8028e1914bb36a5586ee2613b087\n",
            "  Building wheel for sox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sox: filename=sox-1.5.0-py3-none-any.whl size=40036 sha256=92bb4295f3ade0b19836a61ec849e8da84b359951ab301d36e2ae481bfa30560\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/e7/7b/8033be3ec5e4994595d01269fc9657c8fd83a0dcbf8536666a\n",
            "  Building wheel for texterrors (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for texterrors: filename=texterrors-0.5.1-cp310-cp310-linux_x86_64.whl size=1067481 sha256=82731906dcf15375689fdeba938a2cc93d93f13407c4e133b61b559bee753c3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/b3/44/612e60c5d0d1f956213e08ae2c3f73443fde51878c778a9a38\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=e994e8dd8c316256a0541362813756603594457fbaad85cdb10d86916b2d9069\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16257 sha256=95ef8cd78695e9c7ae8b6c6c1224ff95dea345c9cc404f092bdf977a871a9cc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26096 sha256=f2b75dbf1481516fa922782c7c5c4d057f68196845c1a3b05433344701e5311f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built kaldi-python-io sox texterrors wget distance intervaltree\n",
            "Installing collected packages: wget, pydub, plac, distance, braceexpand, xxhash, webdataset, sox, smmap, setproctitle, sentry-sdk, sacremoses, rapidfuzz, pybind11, onnx, marshmallow, loguru, lilcom, libcst, kaldiio, kaldi-python-io, intervaltree, docker-pycreds, dill, cytoolz, resampy, pyloudnorm, multiprocess, Levenshtein, jiwer, hydra-core, gitdb, fiddle, texterrors, nemo-toolkit, lhotse, gitpython, g2p-en, wandb, datasets\n",
            "Successfully installed Levenshtein-0.26.0 braceexpand-0.1.7 cytoolz-1.0.0 datasets-3.0.1 dill-0.3.8 distance-0.1.3 docker-pycreds-0.4.0 fiddle-0.3.0 g2p-en-2.1.0 gitdb-4.0.11 gitpython-3.1.43 hydra-core-1.3.2 intervaltree-3.1.0 jiwer-3.0.4 kaldi-python-io-1.2.2 kaldiio-2.18.0 lhotse-1.27.0 libcst-1.5.0 lilcom-1.8.0 loguru-0.7.2 marshmallow-3.22.0 multiprocess-0.70.16 nemo-toolkit-2.0.0rc1 onnx-1.17.0 plac-1.4.3 pybind11-2.13.6 pydub-0.25.1 pyloudnorm-0.1.1 rapidfuzz-3.10.0 resampy-0.4.3 sacremoses-0.1.1 sentry-sdk-2.16.0 setproctitle-1.3.3 smmap-5.0.1 sox-1.5.0 texterrors-0.5.1 wandb-0.18.3 webdataset-0.2.100 wget-3.2 xxhash-3.5.0\n",
            "Collecting demucs\n",
            "  Cloning https://github.com/facebookresearch/demucs to /tmp/pip-install-myicbvs4/demucs_fdcd5da515554682a035596783916f90\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/demucs /tmp/pip-install-myicbvs4/demucs_fdcd5da515554682a035596783916f90\n",
            "  Resolved https://github.com/facebookresearch/demucs to commit e976d93ecc3865e5757426930257e200846a520a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: demucs\n",
            "  Building wheel for demucs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for demucs: filename=demucs-4.1.0a2-py3-none-any.whl size=83540 sha256=2345678bc6d703442fbc56312bd7af885f0eee6bfbc7faaba0b45f8a8bdf81dd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r9v61s98/wheels/45/e4/ca/7791f04b554e5433713e22900eaf11595e27c454fb65ac30ab\n",
            "Successfully built demucs\n",
            "Installing collected packages: demucs\n",
            "Successfully installed demucs-4.1.0a2\n",
            "Collecting git+https://github.com/oliverguhr/deepmultilingualpunctuation.git\n",
            "  Cloning https://github.com/oliverguhr/deepmultilingualpunctuation.git to /tmp/pip-req-build-9gacf8lx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/oliverguhr/deepmultilingualpunctuation.git /tmp/pip-req-build-9gacf8lx\n",
            "  Resolved https://github.com/oliverguhr/deepmultilingualpunctuation.git to commit 5a0dd7f4fd56687f59405aa8eba1144393d8b74b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from deepmultilingualpunctuation==1.0.1) (4.39.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from deepmultilingualpunctuation==1.0.1) (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation==1.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation==1.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation==1.0.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation==1.0.1) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation==1.0.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation==1.0.1) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation==1.0.1) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation==1.0.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation==1.0.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation==1.0.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation==1.0.1) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation==1.0.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation==1.0.1) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation==1.0.1) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation==1.0.1) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->deepmultilingualpunctuation==1.0.1) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepmultilingualpunctuation==1.0.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepmultilingualpunctuation==1.0.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepmultilingualpunctuation==1.0.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepmultilingualpunctuation==1.0.1) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->deepmultilingualpunctuation==1.0.1) (1.3.0)\n",
            "Building wheels for collected packages: deepmultilingualpunctuation\n",
            "  Building wheel for deepmultilingualpunctuation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepmultilingualpunctuation: filename=deepmultilingualpunctuation-1.0.1-py3-none-any.whl size=5798 sha256=d25e575c84980fa985e8524c470e965809ba3eca7b0b780ce067e63dd48d7ec5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3nnovkvu/wheels/13/14/09/4bbf99cee96e2fbcc7491b5d9087165f0e7f869d276aaa0ec5\n",
            "Successfully built deepmultilingualpunctuation\n",
            "Installing collected packages: deepmultilingualpunctuation\n",
            "Successfully installed deepmultilingualpunctuation-1.0.1\n",
            "Collecting git+https://github.com/MahmoudAshraf97/ctc-forced-aligner.git\n",
            "  Cloning https://github.com/MahmoudAshraf97/ctc-forced-aligner.git to /tmp/pip-req-build-2492qyzh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MahmoudAshraf97/ctc-forced-aligner.git /tmp/pip-req-build-2492qyzh\n",
            "  Resolved https://github.com/MahmoudAshraf97/ctc-forced-aligner.git to commit 553b0fd968dc85d81bda16971edfa39493979ebc\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ctc-forced-aligner==0.2) (3.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ctc-forced-aligner==0.2) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from ctc-forced-aligner==0.2) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.34 in /usr/local/lib/python3.10/dist-packages (from ctc-forced-aligner==0.2) (4.39.3)\n",
            "Collecting Unidecode (from ctc-forced-aligner==0.2)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->ctc-forced-aligner==0.2) (4.66.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ctc-forced-aligner==0.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ctc-forced-aligner==0.2) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ctc-forced-aligner==0.2) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ctc-forced-aligner==0.2) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34->ctc-forced-aligner==0.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34->ctc-forced-aligner==0.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34->ctc-forced-aligner==0.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34->ctc-forced-aligner==0.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ctc-forced-aligner==0.2) (1.3.0)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: ctc-forced-aligner\n",
            "  Building wheel for ctc-forced-aligner (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctc-forced-aligner: filename=ctc_forced_aligner-0.2-cp310-cp310-linux_x86_64.whl size=1153523 sha256=220dd77e3d84aa27d17fbfcabd874d01d970ad78dd35464663f44e1684ead42d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jiyjg71o/wheels/1c/98/3b/f4b23220ebc8d8d2357d6d743a779511a220572d2103ab9060\n",
            "Successfully built ctc-forced-aligner\n",
            "Installing collected packages: Unidecode, ctc-forced-aligner\n",
            "Successfully installed Unidecode-1.3.8 ctc-forced-aligner-0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/m-bain/whisperX.git@78dcfaab51005aa703ee21375f81ed31bc248560\n",
        "!pip install \"nemo-toolkit[asr]>=2.dev\"\n",
        "!pip install --no-deps git+https://github.com/facebookresearch/demucs#egg=demucs\n",
        "!pip install git+https://github.com/oliverguhr/deepmultilingualpunctuation.git\n",
        "!pip install git+https://github.com/MahmoudAshraf97/ctc-forced-aligner.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzhncHP0ytbQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wget\n",
        "from omegaconf import OmegaConf\n",
        "import json\n",
        "import shutil\n",
        "import torch\n",
        "import torchaudio\n",
        "from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "import re\n",
        "import logging\n",
        "import nltk\n",
        "from whisperx.utils import LANGUAGES, TO_LANGUAGE_CODE\n",
        "from ctc_forced_aligner import (\n",
        "    load_alignment_model,\n",
        "    generate_emissions,\n",
        "    preprocess_text,\n",
        "    get_alignments,\n",
        "    get_spans,\n",
        "    postprocess_results,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsUt3SwyhjD"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se6Hc7CZygxu"
      },
      "outputs": [],
      "source": [
        "punct_model_langs = [\n",
        "    \"en\",\n",
        "    \"fr\",\n",
        "    \"de\",\n",
        "    \"es\",\n",
        "    \"it\",\n",
        "    \"nl\",\n",
        "    \"pt\",\n",
        "    \"bg\",\n",
        "    \"pl\",\n",
        "    \"cs\",\n",
        "    \"sk\",\n",
        "    \"sl\",\n",
        "]\n",
        "langs_to_iso = {\n",
        "    \"af\": \"afr\",\n",
        "    \"am\": \"amh\",\n",
        "    \"ar\": \"ara\",\n",
        "    \"as\": \"asm\",\n",
        "    \"az\": \"aze\",\n",
        "    \"ba\": \"bak\",\n",
        "    \"be\": \"bel\",\n",
        "    \"bg\": \"bul\",\n",
        "    \"bn\": \"ben\",\n",
        "    \"bo\": \"tib\",\n",
        "    \"br\": \"bre\",\n",
        "    \"bs\": \"bos\",\n",
        "    \"ca\": \"cat\",\n",
        "    \"cs\": \"cze\",\n",
        "    \"cy\": \"wel\",\n",
        "    \"da\": \"dan\",\n",
        "    \"de\": \"ger\",\n",
        "    \"el\": \"gre\",\n",
        "    \"en\": \"eng\",\n",
        "    \"es\": \"spa\",\n",
        "    \"et\": \"est\",\n",
        "    \"eu\": \"baq\",\n",
        "    \"fa\": \"per\",\n",
        "    \"fi\": \"fin\",\n",
        "    \"fo\": \"fao\",\n",
        "    \"fr\": \"fre\",\n",
        "    \"gl\": \"glg\",\n",
        "    \"gu\": \"guj\",\n",
        "    \"ha\": \"hau\",\n",
        "    \"haw\": \"haw\",\n",
        "    \"he\": \"heb\",\n",
        "    \"hi\": \"hin\",\n",
        "    \"hr\": \"hrv\",\n",
        "    \"ht\": \"hat\",\n",
        "    \"hu\": \"hun\",\n",
        "    \"hy\": \"arm\",\n",
        "    \"id\": \"ind\",\n",
        "    \"is\": \"ice\",\n",
        "    \"it\": \"ita\",\n",
        "    \"ja\": \"jpn\",\n",
        "    \"jw\": \"jav\",\n",
        "    \"ka\": \"geo\",\n",
        "    \"kk\": \"kaz\",\n",
        "    \"km\": \"khm\",\n",
        "    \"kn\": \"kan\",\n",
        "    \"ko\": \"kor\",\n",
        "    \"la\": \"lat\",\n",
        "    \"lb\": \"ltz\",\n",
        "    \"ln\": \"lin\",\n",
        "    \"lo\": \"lao\",\n",
        "    \"lt\": \"lit\",\n",
        "    \"lv\": \"lav\",\n",
        "    \"mg\": \"mlg\",\n",
        "    \"mi\": \"mao\",\n",
        "    \"mk\": \"mac\",\n",
        "    \"ml\": \"mal\",\n",
        "    \"mn\": \"mon\",\n",
        "    \"mr\": \"mar\",\n",
        "    \"ms\": \"may\",\n",
        "    \"mt\": \"mlt\",\n",
        "    \"my\": \"bur\",\n",
        "    \"ne\": \"nep\",\n",
        "    \"nl\": \"dut\",\n",
        "    \"nn\": \"nno\",\n",
        "    \"no\": \"nor\",\n",
        "    \"oc\": \"oci\",\n",
        "    \"pa\": \"pan\",\n",
        "    \"pl\": \"pol\",\n",
        "    \"ps\": \"pus\",\n",
        "    \"pt\": \"por\",\n",
        "    \"ro\": \"rum\",\n",
        "    \"ru\": \"rus\",\n",
        "    \"sa\": \"san\",\n",
        "    \"sd\": \"snd\",\n",
        "    \"si\": \"sin\",\n",
        "    \"sk\": \"slo\",\n",
        "    \"sl\": \"slv\",\n",
        "    \"sn\": \"sna\",\n",
        "    \"so\": \"som\",\n",
        "    \"sq\": \"alb\",\n",
        "    \"sr\": \"srp\",\n",
        "    \"su\": \"sun\",\n",
        "    \"sv\": \"swe\",\n",
        "    \"sw\": \"swa\",\n",
        "    \"ta\": \"tam\",\n",
        "    \"te\": \"tel\",\n",
        "    \"tg\": \"tgk\",\n",
        "    \"th\": \"tha\",\n",
        "    \"tk\": \"tuk\",\n",
        "    \"tl\": \"tgl\",\n",
        "    \"tr\": \"tur\",\n",
        "    \"tt\": \"tat\",\n",
        "    \"uk\": \"ukr\",\n",
        "    \"ur\": \"urd\",\n",
        "    \"uz\": \"uzb\",\n",
        "    \"vi\": \"vie\",\n",
        "    \"yi\": \"yid\",\n",
        "    \"yo\": \"yor\",\n",
        "    \"yue\": \"yue\",\n",
        "    \"zh\": \"chi\",\n",
        "}\n",
        "\n",
        "\n",
        "whisper_langs = sorted(LANGUAGES.keys()) + sorted(\n",
        "    [k.title() for k in TO_LANGUAGE_CODE.keys()]\n",
        ")\n",
        "\n",
        "\n",
        "def create_config(output_dir):\n",
        "    DOMAIN_TYPE = \"telephonic\"  # Can be meeting, telephonic, or general based on domain type of the audio file\n",
        "    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
        "    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
        "    MODEL_CONFIG = os.path.join(output_dir, CONFIG_FILE_NAME)\n",
        "    if not os.path.exists(MODEL_CONFIG):\n",
        "        MODEL_CONFIG = wget.download(CONFIG_URL, output_dir)\n",
        "\n",
        "    config = OmegaConf.load(MODEL_CONFIG)\n",
        "\n",
        "    data_dir = os.path.join(output_dir, \"data\")\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    meta = {\n",
        "        \"audio_filepath\": os.path.join(output_dir, \"mono_file.wav\"),\n",
        "        \"offset\": 0,\n",
        "        \"duration\": None,\n",
        "        \"label\": \"infer\",\n",
        "        \"text\": \"-\",\n",
        "        \"rttm_filepath\": None,\n",
        "        \"uem_filepath\": None,\n",
        "    }\n",
        "    with open(os.path.join(data_dir, \"input_manifest.json\"), \"w\") as fp:\n",
        "        json.dump(meta, fp)\n",
        "        fp.write(\"\\n\")\n",
        "\n",
        "    pretrained_vad = \"vad_multilingual_marblenet\"\n",
        "    pretrained_speaker_model = \"titanet_large\"\n",
        "    config.num_workers = 0  # Workaround for multiprocessing hanging with ipython issue\n",
        "    config.diarizer.manifest_filepath = os.path.join(data_dir, \"input_manifest.json\")\n",
        "    config.diarizer.out_dir = (\n",
        "        output_dir  # Directory to store intermediate files and prediction outputs\n",
        "    )\n",
        "\n",
        "    config.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n",
        "    config.diarizer.oracle_vad = (\n",
        "        False  # compute VAD provided with model_path to vad config\n",
        "    )\n",
        "    config.diarizer.clustering.parameters.oracle_num_speakers = False\n",
        "\n",
        "    # Here, we use our in-house pretrained NeMo VAD model\n",
        "    config.diarizer.vad.model_path = pretrained_vad\n",
        "    config.diarizer.vad.parameters.onset = 0.8\n",
        "    config.diarizer.vad.parameters.offset = 0.6\n",
        "    config.diarizer.vad.parameters.pad_offset = -0.05\n",
        "    config.diarizer.msdd_model.model_path = (\n",
        "        \"diar_msdd_telephonic\"  # Telephonic speaker diarization model\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_word_ts_anchor(s, e, option=\"start\"):\n",
        "    if option == \"end\":\n",
        "        return e\n",
        "    elif option == \"mid\":\n",
        "        return (s + e) / 2\n",
        "    return s\n",
        "\n",
        "\n",
        "def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option=\"start\"):\n",
        "    s, e, sp = spk_ts[0]\n",
        "    wrd_pos, turn_idx = 0, 0\n",
        "    wrd_spk_mapping = []\n",
        "    for wrd_dict in wrd_ts:\n",
        "        ws, we, wrd = (\n",
        "            int(wrd_dict[\"start\"] * 1000),\n",
        "            int(wrd_dict[\"end\"] * 1000),\n",
        "            wrd_dict[\"text\"],\n",
        "        )\n",
        "        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n",
        "        while wrd_pos > float(e):\n",
        "            turn_idx += 1\n",
        "            turn_idx = min(turn_idx, len(spk_ts) - 1)\n",
        "            s, e, sp = spk_ts[turn_idx]\n",
        "            if turn_idx == len(spk_ts) - 1:\n",
        "                e = get_word_ts_anchor(ws, we, option=\"end\")\n",
        "        wrd_spk_mapping.append(\n",
        "            {\"word\": wrd, \"start_time\": ws, \"end_time\": we, \"speaker\": sp}\n",
        "        )\n",
        "    return wrd_spk_mapping\n",
        "\n",
        "\n",
        "sentence_ending_punctuations = \".?!\"\n",
        "\n",
        "\n",
        "def get_first_word_idx_of_sentence(word_idx, word_list, speaker_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    left_idx = word_idx\n",
        "    while (\n",
        "        left_idx > 0\n",
        "        and word_idx - left_idx < max_words\n",
        "        and speaker_list[left_idx - 1] == speaker_list[left_idx]\n",
        "        and not is_word_sentence_end(left_idx - 1)\n",
        "    ):\n",
        "        left_idx -= 1\n",
        "\n",
        "    return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
        "\n",
        "\n",
        "def get_last_word_idx_of_sentence(word_idx, word_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    right_idx = word_idx\n",
        "    while (\n",
        "        right_idx < len(word_list) - 1\n",
        "        and right_idx - word_idx < max_words\n",
        "        and not is_word_sentence_end(right_idx)\n",
        "    ):\n",
        "        right_idx += 1\n",
        "\n",
        "    return (\n",
        "        right_idx\n",
        "        if right_idx == len(word_list) - 1 or is_word_sentence_end(right_idx)\n",
        "        else -1\n",
        "    )\n",
        "\n",
        "\n",
        "def get_realigned_ws_mapping_with_punctuation(\n",
        "    word_speaker_mapping, max_words_in_sentence=50\n",
        "):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0\n",
        "        and word_speaker_mapping[x][\"word\"][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    wsp_len = len(word_speaker_mapping)\n",
        "\n",
        "    words_list, speaker_list = [], []\n",
        "    for k, line_dict in enumerate(word_speaker_mapping):\n",
        "        word, speaker = line_dict[\"word\"], line_dict[\"speaker\"]\n",
        "        words_list.append(word)\n",
        "        speaker_list.append(speaker)\n",
        "\n",
        "    k = 0\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k]\n",
        "        if (\n",
        "            k < wsp_len - 1\n",
        "            and speaker_list[k] != speaker_list[k + 1]\n",
        "            and not is_word_sentence_end(k)\n",
        "        ):\n",
        "            left_idx = get_first_word_idx_of_sentence(\n",
        "                k, words_list, speaker_list, max_words_in_sentence\n",
        "            )\n",
        "            right_idx = (\n",
        "                get_last_word_idx_of_sentence(\n",
        "                    k, words_list, max_words_in_sentence - k + left_idx - 1\n",
        "                )\n",
        "                if left_idx > -1\n",
        "                else -1\n",
        "            )\n",
        "            if min(left_idx, right_idx) == -1:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            spk_labels = speaker_list[left_idx : right_idx + 1]\n",
        "            mod_speaker = max(set(spk_labels), key=spk_labels.count)\n",
        "            if spk_labels.count(mod_speaker) < len(spk_labels) // 2:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            speaker_list[left_idx : right_idx + 1] = [mod_speaker] * (\n",
        "                right_idx - left_idx + 1\n",
        "            )\n",
        "            k = right_idx\n",
        "\n",
        "        k += 1\n",
        "\n",
        "    k, realigned_list = 0, []\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k].copy()\n",
        "        line_dict[\"speaker\"] = speaker_list[k]\n",
        "        realigned_list.append(line_dict)\n",
        "        k += 1\n",
        "\n",
        "    return realigned_list\n",
        "\n",
        "\n",
        "def get_sentences_speaker_mapping(word_speaker_mapping, spk_ts):\n",
        "    sentence_checker = nltk.tokenize.PunktSentenceTokenizer().text_contains_sentbreak\n",
        "    s, e, spk = spk_ts[0]\n",
        "    prev_spk = spk\n",
        "\n",
        "    snts = []\n",
        "    snt = {\"speaker\": f\"Speaker {spk}\", \"start_time\": s, \"end_time\": e, \"text\": \"\"}\n",
        "\n",
        "    for wrd_dict in word_speaker_mapping:\n",
        "        wrd, spk = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
        "        s, e = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
        "        if spk != prev_spk or sentence_checker(snt[\"text\"] + \" \" + wrd):\n",
        "            snts.append(snt)\n",
        "            snt = {\n",
        "                \"speaker\": f\"Speaker {spk}\",\n",
        "                \"start_time\": s,\n",
        "                \"end_time\": e,\n",
        "                \"text\": \"\",\n",
        "            }\n",
        "        else:\n",
        "            snt[\"end_time\"] = e\n",
        "        snt[\"text\"] += wrd + \" \"\n",
        "        prev_spk = spk\n",
        "\n",
        "    snts.append(snt)\n",
        "    return snts\n",
        "\n",
        "def get_speaker_aware_transcript(sentences_speaker_mapping, f):\n",
        "    previous_speaker = sentences_speaker_mapping[0][\"speaker\"]\n",
        "    f.write(f\"{previous_speaker}: \")\n",
        "\n",
        "    for sentence_dict in sentences_speaker_mapping:\n",
        "        speaker = sentence_dict[\"speaker\"]\n",
        "        sentence = sentence_dict[\"text\"]\n",
        "\n",
        "        # If this speaker doesn't match the previous one, start a new paragraph\n",
        "        if speaker != previous_speaker:\n",
        "            f.write(f\"\\n\\n{speaker}: \")\n",
        "            previous_speaker = speaker\n",
        "\n",
        "        # No matter what, write the current sentence\n",
        "        f.write(sentence + \" \")\n",
        "\n",
        "def get_sentences_speaker_mapping_time(word_speaker_mapping, spk_ts):\n",
        "    # Initialize sentence tokenizer\n",
        "    sentence_checker = nltk.tokenize.PunktSentenceTokenizer().text_contains_sentbreak\n",
        "\n",
        "    # Initialize variables for speaker and first sentence\n",
        "    s, e, spk = spk_ts[0]  # Start time, end time, first speaker\n",
        "    prev_spk = spk  # Track previous speaker\n",
        "\n",
        "    # This list will hold the sentence-level speaker mappings\n",
        "    snts = []\n",
        "    # Current sentence dict\n",
        "    snt = {\n",
        "        \"speaker\": f\"Speaker {spk}\",\n",
        "        \"start_time\": s,  # Set start_time for the first speaker\n",
        "        \"end_time\": e,\n",
        "        \"text\": \"\",\n",
        "    }\n",
        "\n",
        "    # Store the first occurrence of each speaker's timestamp\n",
        "    first_speaker_timestamp = {}\n",
        "\n",
        "    # Loop over the word-to-speaker mapping\n",
        "    for wrd_dict in word_speaker_mapping:\n",
        "        wrd, spk = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
        "        s, e = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
        "\n",
        "        # Check for a new speaker or sentence boundary\n",
        "        if spk != prev_spk or sentence_checker(snt[\"text\"] + \" \" + wrd):\n",
        "            # Append the completed sentence with the correct end time\n",
        "            snts.append(snt)\n",
        "\n",
        "            # Start a new sentence for the current speaker\n",
        "            snt = {\n",
        "                \"speaker\": f\"Speaker {spk}\",\n",
        "                \"start_time\": s,  # Use the current word's start time as the new sentence start time\n",
        "                \"end_time\": e,\n",
        "                \"text\": \"\",\n",
        "            }\n",
        "        else:\n",
        "            # If still the same speaker, just update the end time\n",
        "            snt[\"end_time\"] = e\n",
        "\n",
        "        # Add the current word to the sentence text\n",
        "        snt[\"text\"] += wrd + \" \"\n",
        "        # Update the previous speaker\n",
        "        prev_spk = spk\n",
        "\n",
        "    # Append the final sentence\n",
        "    snts.append(snt)\n",
        "\n",
        "    return snts\n",
        "\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "# Helper function to convert milliseconds to readable time (HH:MM:SS)\n",
        "def format_timestamp(ms):\n",
        "    return str(timedelta(milliseconds=ms)).split(\".\")[0]  # Removing microseconds\n",
        "\n",
        "# Function to write the speaker-aware transcript to a file\n",
        "def get_speaker_aware_transcript_time(sentences_speaker_mapping, f):\n",
        "    previous_speaker = sentences_speaker_mapping[0][\"speaker\"]\n",
        "    start_time = sentences_speaker_mapping[0][\"start_time\"]\n",
        "    merged_text = \"\"\n",
        "\n",
        "    for i, sentence_dict in enumerate(sentences_speaker_mapping):\n",
        "        speaker = sentence_dict[\"speaker\"]\n",
        "        sentence = sentence_dict[\"text\"].strip()  # Strip extra spaces\n",
        "        current_start_time = sentence_dict[\"start_time\"]  # Get the start time for the current speaker block\n",
        "        print(f\"Current start time (in ms): {current_start_time}\")\n",
        "\n",
        "        # If the speaker changes or it's the last sentence, write the merged text\n",
        "        if speaker != previous_speaker or i == len(sentences_speaker_mapping) - 1:\n",
        "            # For the last sentence, append it to the merged text\n",
        "            if i == len(sentences_speaker_mapping) - 1 and speaker == previous_speaker:\n",
        "                merged_text += sentence + \" \"\n",
        "\n",
        "            # Write the previous speaker's text with their timestamp\n",
        "            f.write(f\"[{format_timestamp(start_time)}] {previous_speaker}: {merged_text.strip()}\\n\\n\")\n",
        "\n",
        "            # Reset the merged text for the new speaker and update the start time\n",
        "            merged_text = sentence + \" \"\n",
        "            previous_speaker = speaker\n",
        "            start_time = current_start_time\n",
        "        else:\n",
        "            # If it's the same speaker, keep merging their sentences\n",
        "            merged_text += sentence + \" \"\n",
        "\n",
        "    # For the case where the last speaker is still merging, write the final block\n",
        "    if merged_text:\n",
        "        f.write(f\"[{format_timestamp(start_time)}] {previous_speaker}: {merged_text.strip()}\\n\\n\")\n",
        "\n",
        "\n",
        "def format_timestamp(\n",
        "    milliseconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n",
        "):\n",
        "    assert milliseconds >= 0, \"non-negative timestamp expected\"\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n",
        "    return (\n",
        "        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def write_srt(transcript, file):\n",
        "    \"\"\"\n",
        "    Write a transcript to a file in SRT format.\n",
        "\n",
        "    \"\"\"\n",
        "    for i, segment in enumerate(transcript, start=1):\n",
        "        # write srt lines\n",
        "        print(\n",
        "            f\"{i}\\n\"\n",
        "            f\"{format_timestamp(segment['start_time'], always_include_hours=True, decimal_marker=',')} --> \"\n",
        "            f\"{format_timestamp(segment['end_time'], always_include_hours=True, decimal_marker=',')}\\n\"\n",
        "            f\"{segment['speaker']}: {segment['text'].strip().replace('-->', '->')}\\n\",\n",
        "            file=file,\n",
        "            flush=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def find_numeral_symbol_tokens(tokenizer):\n",
        "    numeral_symbol_tokens = [\n",
        "        -1,\n",
        "    ]\n",
        "    for token, token_id in tokenizer.get_vocab().items():\n",
        "        has_numeral_symbol = any(c in \"0123456789%$£\" for c in token)\n",
        "        if has_numeral_symbol:\n",
        "            numeral_symbol_tokens.append(token_id)\n",
        "    return numeral_symbol_tokens\n",
        "\n",
        "\n",
        "def _get_next_start_timestamp(word_timestamps, current_word_index, final_timestamp):\n",
        "    # if current word is the last word\n",
        "    if current_word_index == len(word_timestamps) - 1:\n",
        "        return word_timestamps[current_word_index][\"start\"]\n",
        "\n",
        "    next_word_index = current_word_index + 1\n",
        "    while current_word_index < len(word_timestamps) - 1:\n",
        "        if word_timestamps[next_word_index].get(\"start\") is None:\n",
        "            # if next word doesn't have a start timestamp\n",
        "            # merge it with the current word and delete it\n",
        "            word_timestamps[current_word_index][\"word\"] += (\n",
        "                \" \" + word_timestamps[next_word_index][\"word\"]\n",
        "            )\n",
        "\n",
        "            word_timestamps[next_word_index][\"word\"] = None\n",
        "            next_word_index += 1\n",
        "            if next_word_index == len(word_timestamps):\n",
        "                return final_timestamp\n",
        "\n",
        "        else:\n",
        "            return word_timestamps[next_word_index][\"start\"]\n",
        "\n",
        "\n",
        "def filter_missing_timestamps(\n",
        "    word_timestamps, initial_timestamp=0, final_timestamp=None\n",
        "):\n",
        "    # handle the first and last word\n",
        "    if word_timestamps[0].get(\"start\") is None:\n",
        "        word_timestamps[0][\"start\"] = (\n",
        "            initial_timestamp if initial_timestamp is not None else 0\n",
        "        )\n",
        "        word_timestamps[0][\"end\"] = _get_next_start_timestamp(\n",
        "            word_timestamps, 0, final_timestamp\n",
        "        )\n",
        "\n",
        "    result = [\n",
        "        word_timestamps[0],\n",
        "    ]\n",
        "\n",
        "    for i, ws in enumerate(word_timestamps[1:], start=1):\n",
        "        # if ws doesn't have a start and end\n",
        "        # use the previous end as start and next start as end\n",
        "        if ws.get(\"start\") is None and ws.get(\"word\") is not None:\n",
        "            ws[\"start\"] = word_timestamps[i - 1][\"end\"]\n",
        "            ws[\"end\"] = _get_next_start_timestamp(word_timestamps, i, final_timestamp)\n",
        "\n",
        "        if ws[\"word\"] is not None:\n",
        "            result.append(ws)\n",
        "    return result\n",
        "\n",
        "\n",
        "def cleanup(path: str):\n",
        "    \"\"\"path could either be relative or absolute.\"\"\"\n",
        "    # check if file or directory exists\n",
        "    if os.path.isfile(path) or os.path.islink(path):\n",
        "        # remove file\n",
        "        os.remove(path)\n",
        "    elif os.path.isdir(path):\n",
        "        # remove directory and all its content\n",
        "        shutil.rmtree(path)\n",
        "    else:\n",
        "        raise ValueError(\"Path {} is not a file or dir.\".format(path))\n",
        "\n",
        "\n",
        "def process_language_arg(language: str, model_name: str):\n",
        "    \"\"\"\n",
        "    Process the language argument to make sure it's valid and convert language names to language codes.\n",
        "    \"\"\"\n",
        "    if language is not None:\n",
        "        language = language.lower()\n",
        "    if language not in LANGUAGES:\n",
        "        if language in TO_LANGUAGE_CODE:\n",
        "            language = TO_LANGUAGE_CODE[language]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported language: {language}\")\n",
        "\n",
        "    if model_name.endswith(\".en\") and language != \"en\":\n",
        "        if language is not None:\n",
        "            logging.warning(\n",
        "                f\"{model_name} is an English-only model but received '{language}'; using English instead.\"\n",
        "            )\n",
        "        language = \"en\"\n",
        "    return language\n",
        "\n",
        "\n",
        "def transcribe_batched(\n",
        "    audio_file: str,\n",
        "    language: str,\n",
        "    batch_size: int,\n",
        "    model_name: str,\n",
        "    compute_dtype: str,\n",
        "    suppress_numerals: bool,\n",
        "    device: str,\n",
        "):\n",
        "    import whisperx\n",
        "\n",
        "    # Faster Whisper batched\n",
        "    whisper_model = whisperx.load_model(\n",
        "        model_name,\n",
        "        device,\n",
        "        compute_type=compute_dtype,\n",
        "        asr_options={\"suppress_numerals\": suppress_numerals},\n",
        "    )\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "    result = whisper_model.transcribe(audio, language=language, batch_size=batch_size)\n",
        "    del whisper_model\n",
        "    torch.cuda.empty_cache()\n",
        "    return result[\"segments\"], result[\"language\"], audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7qWQb--1Xcw"
      },
      "source": [
        "# Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONlFrSnD0FOp"
      },
      "outputs": [],
      "source": [
        "# Name of the audio file\n",
        "audio_path = \"/content/output_audio (1).mp3\"\n",
        "\n",
        "# Whether to enable music removal from speech, helps increase diarization quality but uses alot of ram\n",
        "enable_stemming = True\n",
        "\n",
        "# (choose from 'tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large-v3', 'large')\n",
        "whisper_model_name = \"large-v2\"\n",
        "\n",
        "# replaces numerical digits with their pronounciation, increases diarization accuracy\n",
        "suppress_numerals = True\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "language = None  # autodetect language\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-cY1ZEy2KVI"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZS4xXmE2NGP"
      },
      "source": [
        "## Separating music from speech using Demucs\n",
        "\n",
        "---\n",
        "\n",
        "By isolating the vocals from the rest of the audio, it becomes easier to identify and track individual speakers based on the spectral and temporal characteristics of their speech signals. Source separation is just one of many techniques that can be used as a preprocessing step to help improve the accuracy and reliability of the overall diarization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKcgQUrAzsJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632344be-a131-447c-8f1e-b346ed9d76f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Source splitting failed, using original audio file.\n"
          ]
        }
      ],
      "source": [
        "if enable_stemming:\n",
        "    # Isolate vocals from the rest of the audio\n",
        "\n",
        "    return_code = os.system(\n",
        "        f'python3 -m demucs.separate -n htdemucs --two-stems=vocals \"{audio_path}\" -o \"temp_outputs\"'\n",
        "    )\n",
        "\n",
        "    if return_code != 0:\n",
        "        logging.warning(\"Source splitting failed, using original audio file.\")\n",
        "        vocal_target = audio_path\n",
        "    else:\n",
        "        vocal_target = os.path.join(\n",
        "            \"temp_outputs\",\n",
        "            \"htdemucs\",\n",
        "            os.path.splitext(os.path.basename(audio_path))[0],\n",
        "            \"vocals.wav\",\n",
        "        )\n",
        "else:\n",
        "    vocal_target = audio_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYg9VWb22Tz8"
      },
      "source": [
        "## Transcriping audio using Whisper and realligning timestamps using Wav2Vec2\n",
        "---\n",
        "This code uses two different open-source models to transcribe speech and perform forced alignment on the resulting transcription.\n",
        "\n",
        "The first model is called OpenAI Whisper, which is a speech recognition model that can transcribe speech with high accuracy. The code loads the whisper model and uses it to transcribe the vocal_target file.\n",
        "\n",
        "The output of the transcription process is a set of text segments with corresponding timestamps indicating when each segment was spoken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-VKFn530oTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabcc09f-175f-43ac-853c-a2be9dd17c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.4.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (1.00) in first 30s of audio...\n",
            "Suppressing numeral and symbol tokens\n"
          ]
        }
      ],
      "source": [
        "compute_type = \"float16\"\n",
        "# or run on GPU with INT8\n",
        "# compute_type = \"int8_float16\"\n",
        "# or run on CPU with INT8\n",
        "# compute_type = \"int8\"\n",
        "\n",
        "whisper_results, language, audio_waveform = transcribe_batched(\n",
        "    vocal_target,\n",
        "    language,\n",
        "    batch_size,\n",
        "    whisper_model_name,\n",
        "    compute_type,\n",
        "    suppress_numerals,\n",
        "    device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ETeQYimvBmj"
      },
      "source": [
        "## Aligning the transcription with the original audio using Wav2Vec2\n",
        "---\n",
        "The second model used is called wav2vec2, which is a large-scale neural network that is designed to learn representations of speech that are useful for a variety of speech processing tasks, including speech recognition and alignment.\n",
        "\n",
        "The code loads the wav2vec2 alignment model and uses it to align the transcription segments with the original audio signal contained in the vocal_target file. This process involves finding the exact timestamps in the audio signal where each segment was spoken and aligning the text accordingly.\n",
        "\n",
        "By combining the outputs of the two models, the code produces a fully aligned transcription of the speech contained in the vocal_target file. This aligned transcription can be useful for a variety of speech processing tasks, such as speaker diarization, sentiment analysis, and language identification.\n",
        "\n",
        "If there's no Wav2Vec2 model available for your language, word timestamps generated by whisper will be used instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYDB4-L4vBmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fea0dd5-d149-4c33-8178-98b1446aa53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-15 20:17:18 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "alignment_model, alignment_tokenizer = load_alignment_model(\n",
        "    device,\n",
        "    dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ")\n",
        "\n",
        "audio_waveform = (\n",
        "    torch.from_numpy(audio_waveform)\n",
        "    .to(alignment_model.dtype)\n",
        "    .to(alignment_model.device)\n",
        ")\n",
        "\n",
        "emissions, stride = generate_emissions(\n",
        "    alignment_model, audio_waveform, batch_size=batch_size\n",
        ")\n",
        "\n",
        "del alignment_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "full_transcript = \"\".join(segment[\"text\"] for segment in whisper_results)\n",
        "\n",
        "tokens_starred, text_starred = preprocess_text(\n",
        "    full_transcript,\n",
        "    romanize=True,\n",
        "    language=langs_to_iso[language],\n",
        ")\n",
        "\n",
        "segments, scores, blank_token = get_alignments(\n",
        "    emissions,\n",
        "    tokens_starred,\n",
        "    alignment_tokenizer,\n",
        ")\n",
        "\n",
        "spans = get_spans(tokens_starred, segments, blank_token)\n",
        "\n",
        "word_timestamps = postprocess_results(text_starred, spans, stride, scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EEaJPsQ21Rx"
      },
      "source": [
        "## Convert audio to mono for NeMo combatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaZl2Q0QvBmk"
      },
      "outputs": [],
      "source": [
        "ROOT = os.getcwd()\n",
        "temp_path = os.path.join(ROOT, \"temp_outputs\")\n",
        "os.makedirs(temp_path, exist_ok=True)\n",
        "torchaudio.save(\n",
        "    os.path.join(temp_path, \"mono_file.wav\"),\n",
        "    audio_waveform.cpu().unsqueeze(0).float(),\n",
        "    16000,\n",
        "    channels_first=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gkViCf2-CV"
      },
      "source": [
        "## Speaker Diarization using NeMo MSDD Model\n",
        "---\n",
        "This code uses a model called Nvidia NeMo MSDD (Multi-scale Diarization Decoder) to perform speaker diarization on an audio signal. Speaker diarization is the process of separating an audio signal into different segments based on who is speaking at any given time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7jIpBCH02RL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f17d19-20ef-436e-c818-00402efbd10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:12 msdd_models:1097] Loading pretrained diar_msdd_telephonic model from NGC\n",
            "[NeMo I 2024-10-15 20:18:12 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_2.0.0rc1/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
            "[NeMo I 2024-10-15 20:18:12 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_2.0.0rc1/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
            "[NeMo I 2024-10-15 20:18:12 common:826] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-15 20:18:14 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: true\n",
            "    \n",
            "[NeMo W 2024-10-15 20:18:14 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: false\n",
            "    \n",
            "[NeMo W 2024-10-15 20:18:14 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: false\n",
            "    seq_eval_mode: false\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:14 features:305] PADDING: 16\n",
            "[NeMo I 2024-10-15 20:18:14 features:305] PADDING: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-15 20:18:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/core/connectors/save_restore_connector.py:617: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "      return torch.load(model_weights, map_location='cpu')\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:14 save_restore_connector:272] Model EncDecDiarLabelModel was successfully restored from /root/.cache/torch/NeMo/NeMo_2.0.0rc1/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
            "[NeMo I 2024-10-15 20:18:14 features:305] PADDING: 16\n",
            "[NeMo I 2024-10-15 20:18:15 clustering_diarizer:127] Loading pretrained vad_multilingual_marblenet model from NGC\n",
            "[NeMo I 2024-10-15 20:18:15 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_2.0.0rc1/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
            "[NeMo I 2024-10-15 20:18:15 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_2.0.0rc1/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
            "[NeMo I 2024-10-15 20:18:15 common:826] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-15 20:18:16 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 256\n",
            "    shuffle: true\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    tarred_shard_strategy: scatter\n",
            "    augmentor:\n",
            "      shift:\n",
            "        prob: 0.5\n",
            "        min_shift_ms: -10.0\n",
            "        max_shift_ms: 10.0\n",
            "      white_noise:\n",
            "        prob: 0.5\n",
            "        min_level: -90\n",
            "        max_level: -46\n",
            "        norm: true\n",
            "      noise:\n",
            "        prob: 0.5\n",
            "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
            "        min_snr_db: 0\n",
            "        max_snr_db: 30\n",
            "        max_gain_db: 300.0\n",
            "        norm: true\n",
            "      gain:\n",
            "        prob: 0.5\n",
            "        min_gain_dbfs: -10.0\n",
            "        max_gain_dbfs: 10.0\n",
            "        norm: true\n",
            "    num_workers: 16\n",
            "    pin_memory: true\n",
            "    \n",
            "[NeMo W 2024-10-15 20:18:16 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 256\n",
            "    shuffle: false\n",
            "    val_loss_idx: 0\n",
            "    num_workers: 16\n",
            "    pin_memory: true\n",
            "    \n",
            "[NeMo W 2024-10-15 20:18:16 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 128\n",
            "    shuffle: false\n",
            "    test_loss_idx: 0\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:16 features:305] PADDING: 16\n",
            "[NeMo I 2024-10-15 20:18:16 save_restore_connector:272] Model EncDecClassificationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_2.0.0rc1/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
            "[NeMo I 2024-10-15 20:18:16 msdd_models:870] Multiscale Weights: [1, 1, 1, 1, 1]\n",
            "[NeMo I 2024-10-15 20:18:16 msdd_models:871] Clustering Parameters: {\n",
            "        \"oracle_num_speakers\": false,\n",
            "        \"max_num_speakers\": 8,\n",
            "        \"enhanced_count_thres\": 80,\n",
            "        \"max_rp_threshold\": 0.25,\n",
            "        \"sparse_search_volume\": 30,\n",
            "        \"maj_vote_spk_count\": false,\n",
            "        \"chunk_cluster_count\": 50,\n",
            "        \"embeddings_per_chunk\": 10000\n",
            "    }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-15 20:18:16 clustering_diarizer:408] Deleting previous clustering diarizer outputs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:16 speaker_utils:93] Number of files to diarize: 1\n",
            "[NeMo I 2024-10-15 20:18:16 clustering_diarizer:313] Split long audio file to avoid CUDA memory issue\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "splitting manifest: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:16 vad_utils:107] The prepared manifest file exists. Overwriting!\n",
            "[NeMo I 2024-10-15 20:18:16 classification_models:293] Perform streaming frame-level VAD\n",
            "[NeMo I 2024-10-15 20:18:16 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-10-15 20:18:16 collections:741] Dataset successfully loaded with 34 items and total duration provided from manifest is  0.47 hours.\n",
            "[NeMo I 2024-10-15 20:18:16 collections:746] # 34 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rvad:   0%|          | 0/34 [00:00<?, ?it/s][NeMo W 2024-10-15 20:18:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/asr/models/clustering_diarizer.py:226: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "      with autocast():\n",
            "    \n",
            "[NeMo W 2024-10-15 20:18:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/asr/parts/preprocessing/features.py:436: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "      with torch.cuda.amp.autocast(enabled=False):\n",
            "    \n",
            "vad: 100%|██████████| 34/34 [00:06<00:00,  5.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:22 clustering_diarizer:254] Generating predictions with overlapping input segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:39 clustering_diarizer:266] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "creating speech segments: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:40 clustering_diarizer:291] Subsegmentation for embedding extraction: scale0, /content/temp_outputs/speaker_outputs/subsegments_scale0.json\n",
            "[NeMo I 2024-10-15 20:18:40 clustering_diarizer:347] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-10-15 20:18:40 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-10-15 20:18:40 collections:741] Dataset successfully loaded with 1531 items and total duration provided from manifest is  0.56 hours.\n",
            "[NeMo I 2024-10-15 20:18:40 collections:746] # 1531 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[1/5] extract embeddings:   0%|          | 0/24 [00:00<?, ?it/s][NeMo W 2024-10-15 20:18:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/asr/models/clustering_diarizer.py:362: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "      with autocast():\n",
            "    \n",
            "[NeMo W 2024-10-15 20:18:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/asr/parts/submodules/jasper.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "      with torch.cuda.amp.autocast(enabled=False):\n",
            "    \n",
            "[1/5] extract embeddings: 100%|██████████| 24/24 [00:02<00:00,  9.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:43 clustering_diarizer:393] Saved embedding files to /content/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2024-10-15 20:18:43 clustering_diarizer:291] Subsegmentation for embedding extraction: scale1, /content/temp_outputs/speaker_outputs/subsegments_scale1.json\n",
            "[NeMo I 2024-10-15 20:18:43 clustering_diarizer:347] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-10-15 20:18:43 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-10-15 20:18:43 collections:741] Dataset successfully loaded with 1865 items and total duration provided from manifest is  0.58 hours.\n",
            "[NeMo I 2024-10-15 20:18:43 collections:746] # 1865 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2/5] extract embeddings: 100%|██████████| 30/30 [00:02<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:46 clustering_diarizer:393] Saved embedding files to /content/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2024-10-15 20:18:46 clustering_diarizer:291] Subsegmentation for embedding extraction: scale2, /content/temp_outputs/speaker_outputs/subsegments_scale2.json\n",
            "[NeMo I 2024-10-15 20:18:46 clustering_diarizer:347] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-10-15 20:18:46 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-10-15 20:18:46 collections:741] Dataset successfully loaded with 2325 items and total duration provided from manifest is  0.60 hours.\n",
            "[NeMo I 2024-10-15 20:18:46 collections:746] # 2325 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[3/5] extract embeddings: 100%|██████████| 37/37 [00:02<00:00, 13.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:50 clustering_diarizer:393] Saved embedding files to /content/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2024-10-15 20:18:50 clustering_diarizer:291] Subsegmentation for embedding extraction: scale3, /content/temp_outputs/speaker_outputs/subsegments_scale3.json\n",
            "[NeMo I 2024-10-15 20:18:50 clustering_diarizer:347] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-10-15 20:18:50 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-10-15 20:18:50 collections:741] Dataset successfully loaded with 3127 items and total duration provided from manifest is  0.62 hours.\n",
            "[NeMo I 2024-10-15 20:18:50 collections:746] # 3127 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[4/5] extract embeddings: 100%|██████████| 49/49 [00:02<00:00, 16.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:53 clustering_diarizer:393] Saved embedding files to /content/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2024-10-15 20:18:53 clustering_diarizer:291] Subsegmentation for embedding extraction: scale4, /content/temp_outputs/speaker_outputs/subsegments_scale4.json\n",
            "[NeMo I 2024-10-15 20:18:53 clustering_diarizer:347] Extracting embeddings for Diarization\n",
            "[NeMo I 2024-10-15 20:18:54 collections:740] Filtered duration for loading collection is  0.00 hours.\n",
            "[NeMo I 2024-10-15 20:18:54 collections:741] Dataset successfully loaded with 4783 items and total duration provided from manifest is  0.64 hours.\n",
            "[NeMo I 2024-10-15 20:18:54 collections:746] # 4783 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[5/5] extract embeddings: 100%|██████████| 75/75 [00:04<00:00, 17.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:18:59 clustering_diarizer:393] Saved embedding files to /content/temp_outputs/speaker_outputs/embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "clustering: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:19:02 clustering_diarizer:461] Outputs are saved in /content/temp_outputs directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[NeMo W 2024-10-15 20:19:02 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:19:03 msdd_models:966] Loading embedding pickle file of scale:0 at /content/temp_outputs/speaker_outputs/embeddings/subsegments_scale0_embeddings.pkl\n",
            "[NeMo I 2024-10-15 20:19:03 msdd_models:966] Loading embedding pickle file of scale:1 at /content/temp_outputs/speaker_outputs/embeddings/subsegments_scale1_embeddings.pkl\n",
            "[NeMo I 2024-10-15 20:19:03 msdd_models:966] Loading embedding pickle file of scale:2 at /content/temp_outputs/speaker_outputs/embeddings/subsegments_scale2_embeddings.pkl\n",
            "[NeMo I 2024-10-15 20:19:03 msdd_models:966] Loading embedding pickle file of scale:3 at /content/temp_outputs/speaker_outputs/embeddings/subsegments_scale3_embeddings.pkl\n",
            "[NeMo I 2024-10-15 20:19:03 msdd_models:966] Loading embedding pickle file of scale:4 at /content/temp_outputs/speaker_outputs/embeddings/subsegments_scale4_embeddings.pkl\n",
            "[NeMo I 2024-10-15 20:19:03 msdd_models:944] Loading cluster label file from /content/temp_outputs/speaker_outputs/subsegments_scale4_cluster.label\n",
            "[NeMo I 2024-10-15 20:19:04 collections:1067] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2024-10-15 20:19:04 collections:1071] Total 1 session files loaded accounting to # 1 audio clips\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2024-10-15 20:19:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/asr/models/msdd_models.py:1333: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "      with autocast():\n",
            "    \n",
            "100%|██████████| 1/1 [00:00<00:00,  4.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:19:04 msdd_models:1407]      [Threshold: 0.7000] [use_clus_as_main=False] [diar_window=50]\n",
            "[NeMo I 2024-10-15 20:19:04 speaker_utils:93] Number of files to diarize: 1\n",
            "[NeMo I 2024-10-15 20:19:04 speaker_utils:93] Number of files to diarize: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[NeMo W 2024-10-15 20:19:04 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:19:04 speaker_utils:93] Number of files to diarize: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-15 20:19:05 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:19:05 speaker_utils:93] Number of files to diarize: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2024-10-15 20:19:05 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2024-10-15 20:19:05 msdd_models:1435]   \n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Initialize NeMo MSDD diarization model\n",
        "msdd_model = NeuralDiarizer(cfg=create_config(temp_path)).to(\"cuda\")\n",
        "msdd_model.diarize()\n",
        "\n",
        "del msdd_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmkZYaDAEOAg"
      },
      "source": [
        "## Mapping Spekers to Sentences According to Timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E65LUGQe02zw"
      },
      "outputs": [],
      "source": [
        "# Reading timestamps <> Speaker Labels mapping\n",
        "\n",
        "speaker_ts = []\n",
        "with open(os.path.join(temp_path, \"pred_rttms\", \"mono_file.rttm\"), \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line_list = line.split(\" \")\n",
        "        s = int(float(line_list[5]) * 1000)\n",
        "        e = s + int(float(line_list[8]) * 1000)\n",
        "        speaker_ts.append([s, e, int(line_list[11].split(\"_\")[-1])])\n",
        "\n",
        "wsm = get_words_speaker_mapping(word_timestamps, speaker_ts, \"start\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ruxc8S1EXtW"
      },
      "source": [
        "## Realligning Speech segments using Punctuation\n",
        "---\n",
        "\n",
        "This code provides a method for disambiguating speaker labels in cases where a sentence is split between two different speakers. It uses punctuation markings to determine the dominant speaker for each sentence in the transcription.\n",
        "\n",
        "```\n",
        "Speaker A: It's got to come from somewhere else. Yeah, that one's also fun because you know the lows are\n",
        "Speaker B: going to suck, right? So it's actually it hits you on both sides.\n",
        "```\n",
        "\n",
        "For example, if a sentence is split between two speakers, the code takes the mode of speaker labels for each word in the sentence, and uses that speaker label for the whole sentence. This can help to improve the accuracy of speaker diarization, especially in cases where the Whisper model may not take fine utterances like \"hmm\" and \"yeah\" into account, but the Diarization Model (Nemo) may include them, leading to inconsistent results.\n",
        "\n",
        "The code also handles cases where one speaker is giving a monologue while other speakers are making occasional comments in the background. It ignores the comments and assigns the entire monologue to the speaker who is speaking the majority of the time. This provides a robust and reliable method for realigning speech segments to their respective speakers based on punctuation in the transcription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgfC5hA41BXu"
      },
      "outputs": [],
      "source": [
        "if language in punct_model_langs:\n",
        "    # restoring punctuation in the transcript to help realign the sentences\n",
        "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
        "\n",
        "    words_list = list(map(lambda x: x[\"word\"], wsm))\n",
        "\n",
        "    labled_words = punct_model.predict(words_list,chunk_size=230)\n",
        "\n",
        "    ending_puncts = \".?!\"\n",
        "    model_puncts = \".,;:!?\"\n",
        "\n",
        "    # We don't want to punctuate U.S.A. with a period. Right?\n",
        "    is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
        "\n",
        "    for word_dict, labeled_tuple in zip(wsm, labled_words):\n",
        "        word = word_dict[\"word\"]\n",
        "        if (\n",
        "            word\n",
        "            and labeled_tuple[1] in ending_puncts\n",
        "            and (word[-1] not in model_puncts or is_acronym(word))\n",
        "        ):\n",
        "            word += labeled_tuple[1]\n",
        "            if word.endswith(\"..\"):\n",
        "                word = word.rstrip(\".\")\n",
        "            word_dict[\"word\"] = word\n",
        "\n",
        "else:\n",
        "    logging.warning(\n",
        "        f\"Punctuation restoration is not available for {language} language. Using the original punctuation.\"\n",
        "    )\n",
        "\n",
        "wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
        "ssm = get_sentences_speaker_mapping_time(wsm, speaker_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF2QAtLOFvwZ"
      },
      "source": [
        "## Cleanup and Exporing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFTyKI6B1MI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7299d7a3-41a1-43b1-8003-325da7c318a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current start time (in ms): 2540\n",
            "Current start time (in ms): 4720\n",
            "Current start time (in ms): 9620\n",
            "Current start time (in ms): 18920\n",
            "Current start time (in ms): 26040\n",
            "Current start time (in ms): 28280\n",
            "Current start time (in ms): 33780\n",
            "Current start time (in ms): 38300\n",
            "Current start time (in ms): 42680\n",
            "Current start time (in ms): 45940\n",
            "Current start time (in ms): 51680\n",
            "Current start time (in ms): 53260\n",
            "Current start time (in ms): 55360\n",
            "Current start time (in ms): 58160\n",
            "Current start time (in ms): 68180\n",
            "Current start time (in ms): 69560\n",
            "Current start time (in ms): 75160\n",
            "Current start time (in ms): 81720\n",
            "Current start time (in ms): 85100\n",
            "Current start time (in ms): 87040\n",
            "Current start time (in ms): 87580\n",
            "Current start time (in ms): 91640\n",
            "Current start time (in ms): 92420\n",
            "Current start time (in ms): 95300\n",
            "Current start time (in ms): 100200\n",
            "Current start time (in ms): 105160\n",
            "Current start time (in ms): 109020\n",
            "Current start time (in ms): 111760\n",
            "Current start time (in ms): 117040\n",
            "Current start time (in ms): 122580\n",
            "Current start time (in ms): 124020\n",
            "Current start time (in ms): 130800\n",
            "Current start time (in ms): 136040\n",
            "Current start time (in ms): 145620\n",
            "Current start time (in ms): 153320\n",
            "Current start time (in ms): 155540\n",
            "Current start time (in ms): 156760\n",
            "Current start time (in ms): 159800\n",
            "Current start time (in ms): 168500\n",
            "Current start time (in ms): 170840\n",
            "Current start time (in ms): 181520\n",
            "Current start time (in ms): 187020\n",
            "Current start time (in ms): 191920\n",
            "Current start time (in ms): 202160\n",
            "Current start time (in ms): 211660\n",
            "Current start time (in ms): 219760\n",
            "Current start time (in ms): 228940\n",
            "Current start time (in ms): 234180\n",
            "Current start time (in ms): 240320\n",
            "Current start time (in ms): 254420\n",
            "Current start time (in ms): 259260\n",
            "Current start time (in ms): 275380\n",
            "Current start time (in ms): 278500\n",
            "Current start time (in ms): 282000\n",
            "Current start time (in ms): 290180\n",
            "Current start time (in ms): 291900\n",
            "Current start time (in ms): 298820\n",
            "Current start time (in ms): 301820\n",
            "Current start time (in ms): 303240\n",
            "Current start time (in ms): 306420\n",
            "Current start time (in ms): 313300\n",
            "Current start time (in ms): 320680\n",
            "Current start time (in ms): 325260\n",
            "Current start time (in ms): 332940\n",
            "Current start time (in ms): 335140\n",
            "Current start time (in ms): 337080\n",
            "Current start time (in ms): 339360\n",
            "Current start time (in ms): 345060\n",
            "Current start time (in ms): 350020\n",
            "Current start time (in ms): 352400\n",
            "Current start time (in ms): 353300\n",
            "Current start time (in ms): 356160\n",
            "Current start time (in ms): 359700\n",
            "Current start time (in ms): 364920\n",
            "Current start time (in ms): 373020\n",
            "Current start time (in ms): 374640\n",
            "Current start time (in ms): 383600\n",
            "Current start time (in ms): 392360\n",
            "Current start time (in ms): 397680\n",
            "Current start time (in ms): 402880\n",
            "Current start time (in ms): 406140\n",
            "Current start time (in ms): 412000\n",
            "Current start time (in ms): 416660\n",
            "Current start time (in ms): 422340\n",
            "Current start time (in ms): 431920\n",
            "Current start time (in ms): 454460\n",
            "Current start time (in ms): 460720\n",
            "Current start time (in ms): 462380\n",
            "Current start time (in ms): 463560\n",
            "Current start time (in ms): 465460\n",
            "Current start time (in ms): 466740\n",
            "Current start time (in ms): 493820\n",
            "Current start time (in ms): 496860\n",
            "Current start time (in ms): 502700\n",
            "Current start time (in ms): 504040\n",
            "Current start time (in ms): 505500\n",
            "Current start time (in ms): 510400\n",
            "Current start time (in ms): 511000\n",
            "Current start time (in ms): 517200\n",
            "Current start time (in ms): 522840\n",
            "Current start time (in ms): 526780\n",
            "Current start time (in ms): 529840\n",
            "Current start time (in ms): 532860\n",
            "Current start time (in ms): 540000\n",
            "Current start time (in ms): 552400\n",
            "Current start time (in ms): 556540\n",
            "Current start time (in ms): 568820\n",
            "Current start time (in ms): 581360\n",
            "Current start time (in ms): 584880\n",
            "Current start time (in ms): 586500\n",
            "Current start time (in ms): 598280\n",
            "Current start time (in ms): 617700\n",
            "Current start time (in ms): 619600\n",
            "Current start time (in ms): 633340\n",
            "Current start time (in ms): 635140\n",
            "Current start time (in ms): 637540\n",
            "Current start time (in ms): 647740\n",
            "Current start time (in ms): 649300\n",
            "Current start time (in ms): 651660\n",
            "Current start time (in ms): 657280\n",
            "Current start time (in ms): 659060\n",
            "Current start time (in ms): 665500\n",
            "Current start time (in ms): 673060\n",
            "Current start time (in ms): 680080\n",
            "Current start time (in ms): 693120\n",
            "Current start time (in ms): 696660\n",
            "Current start time (in ms): 701000\n",
            "Current start time (in ms): 704580\n",
            "Current start time (in ms): 710880\n",
            "Current start time (in ms): 712060\n",
            "Current start time (in ms): 714160\n",
            "Current start time (in ms): 714920\n",
            "Current start time (in ms): 716880\n",
            "Current start time (in ms): 719920\n",
            "Current start time (in ms): 727300\n",
            "Current start time (in ms): 729240\n",
            "Current start time (in ms): 730960\n",
            "Current start time (in ms): 732960\n",
            "Current start time (in ms): 734860\n",
            "Current start time (in ms): 737260\n",
            "Current start time (in ms): 741540\n",
            "Current start time (in ms): 748020\n",
            "Current start time (in ms): 754680\n",
            "Current start time (in ms): 759300\n",
            "Current start time (in ms): 764020\n",
            "Current start time (in ms): 771500\n",
            "Current start time (in ms): 775860\n",
            "Current start time (in ms): 785660\n",
            "Current start time (in ms): 787680\n",
            "Current start time (in ms): 790020\n",
            "Current start time (in ms): 794280\n",
            "Current start time (in ms): 795440\n",
            "Current start time (in ms): 809260\n",
            "Current start time (in ms): 812260\n",
            "Current start time (in ms): 813320\n",
            "Current start time (in ms): 813400\n",
            "Current start time (in ms): 819660\n",
            "Current start time (in ms): 827200\n",
            "Current start time (in ms): 833080\n",
            "Current start time (in ms): 841600\n",
            "Current start time (in ms): 850200\n",
            "Current start time (in ms): 860680\n",
            "Current start time (in ms): 864640\n",
            "Current start time (in ms): 884280\n",
            "Current start time (in ms): 890560\n",
            "Current start time (in ms): 891320\n",
            "Current start time (in ms): 893280\n",
            "Current start time (in ms): 901000\n",
            "Current start time (in ms): 907280\n",
            "Current start time (in ms): 908460\n",
            "Current start time (in ms): 914920\n",
            "Current start time (in ms): 924400\n",
            "Current start time (in ms): 932280\n",
            "Current start time (in ms): 937020\n",
            "Current start time (in ms): 949580\n",
            "Current start time (in ms): 977840\n",
            "Current start time (in ms): 981840\n",
            "Current start time (in ms): 985160\n",
            "Current start time (in ms): 986820\n",
            "Current start time (in ms): 994920\n",
            "Current start time (in ms): 1002160\n",
            "Current start time (in ms): 1008700\n",
            "Current start time (in ms): 1013340\n",
            "Current start time (in ms): 1019860\n",
            "Current start time (in ms): 1028460\n",
            "Current start time (in ms): 1034500\n",
            "Current start time (in ms): 1045700\n",
            "Current start time (in ms): 1047339\n",
            "Current start time (in ms): 1050540\n",
            "Current start time (in ms): 1062460\n",
            "Current start time (in ms): 1063300\n",
            "Current start time (in ms): 1064160\n",
            "Current start time (in ms): 1071760\n",
            "Current start time (in ms): 1080480\n",
            "Current start time (in ms): 1085000\n",
            "Current start time (in ms): 1086600\n",
            "Current start time (in ms): 1088640\n",
            "Current start time (in ms): 1093460\n",
            "Current start time (in ms): 1099180\n",
            "Current start time (in ms): 1105800\n",
            "Current start time (in ms): 1112760\n",
            "Current start time (in ms): 1120620\n",
            "Current start time (in ms): 1129980\n",
            "Current start time (in ms): 1143200\n",
            "Current start time (in ms): 1155560\n",
            "Current start time (in ms): 1169140\n",
            "Current start time (in ms): 1176600\n",
            "Current start time (in ms): 1178020\n",
            "Current start time (in ms): 1181880\n",
            "Current start time (in ms): 1187620\n",
            "Current start time (in ms): 1189660\n",
            "Current start time (in ms): 1191280\n",
            "Current start time (in ms): 1192760\n",
            "Current start time (in ms): 1194620\n",
            "Current start time (in ms): 1196060\n",
            "Current start time (in ms): 1201160\n",
            "Current start time (in ms): 1209260\n",
            "Current start time (in ms): 1218460\n",
            "Current start time (in ms): 1221100\n",
            "Current start time (in ms): 1222000\n",
            "Current start time (in ms): 1223040\n",
            "Current start time (in ms): 1224840\n",
            "Current start time (in ms): 1225800\n",
            "Current start time (in ms): 1235100\n",
            "Current start time (in ms): 1240860\n",
            "Current start time (in ms): 1243220\n",
            "Current start time (in ms): 1246140\n",
            "Current start time (in ms): 1253700\n",
            "Current start time (in ms): 1259520\n",
            "Current start time (in ms): 1269020\n",
            "Current start time (in ms): 1271720\n",
            "Current start time (in ms): 1276980\n",
            "Current start time (in ms): 1288080\n",
            "Current start time (in ms): 1288740\n",
            "Current start time (in ms): 1290660\n",
            "Current start time (in ms): 1296820\n",
            "Current start time (in ms): 1300080\n",
            "Current start time (in ms): 1309640\n",
            "Current start time (in ms): 1314980\n",
            "Current start time (in ms): 1317740\n",
            "Current start time (in ms): 1319840\n",
            "Current start time (in ms): 1325140\n",
            "Current start time (in ms): 1326540\n",
            "Current start time (in ms): 1334360\n",
            "Current start time (in ms): 1335280\n",
            "Current start time (in ms): 1344600\n",
            "Current start time (in ms): 1347280\n",
            "Current start time (in ms): 1351000\n",
            "Current start time (in ms): 1352940\n",
            "Current start time (in ms): 1354300\n",
            "Current start time (in ms): 1359060\n",
            "Current start time (in ms): 1363480\n",
            "Current start time (in ms): 1366720\n",
            "Current start time (in ms): 1372080\n",
            "Current start time (in ms): 1373480\n",
            "Current start time (in ms): 1375640\n",
            "Current start time (in ms): 1376100\n",
            "Current start time (in ms): 1377180\n",
            "Current start time (in ms): 1377500\n",
            "Current start time (in ms): 1378660\n",
            "Current start time (in ms): 1382300\n",
            "Current start time (in ms): 1382860\n",
            "Current start time (in ms): 1393060\n",
            "Current start time (in ms): 1403740\n",
            "Current start time (in ms): 1414580\n",
            "Current start time (in ms): 1419880\n",
            "Current start time (in ms): 1424300\n",
            "Current start time (in ms): 1428920\n",
            "Current start time (in ms): 1430240\n",
            "Current start time (in ms): 1433860\n",
            "Current start time (in ms): 1438260\n",
            "Current start time (in ms): 1448220\n",
            "Current start time (in ms): 1458420\n",
            "Current start time (in ms): 1459480\n",
            "Current start time (in ms): 1463500\n",
            "Current start time (in ms): 1466220\n",
            "Current start time (in ms): 1474000\n",
            "Current start time (in ms): 1481080\n",
            "Current start time (in ms): 1481620\n",
            "Current start time (in ms): 1481980\n",
            "Current start time (in ms): 1484880\n",
            "Current start time (in ms): 1492560\n",
            "Current start time (in ms): 1498740\n",
            "Current start time (in ms): 1502220\n",
            "Current start time (in ms): 1506020\n",
            "Current start time (in ms): 1527940\n",
            "Current start time (in ms): 1533760\n",
            "Current start time (in ms): 1540660\n",
            "Current start time (in ms): 1553100\n",
            "Current start time (in ms): 1553820\n",
            "Current start time (in ms): 1563720\n",
            "Current start time (in ms): 1564680\n",
            "Current start time (in ms): 1570460\n",
            "Current start time (in ms): 1579660\n",
            "Current start time (in ms): 1591780\n",
            "Current start time (in ms): 1596680\n",
            "Current start time (in ms): 1599600\n",
            "Current start time (in ms): 1600580\n",
            "Current start time (in ms): 1610800\n",
            "Current start time (in ms): 1612300\n",
            "Current start time (in ms): 1614940\n",
            "Current start time (in ms): 1616180\n",
            "Current start time (in ms): 1617760\n",
            "Current start time (in ms): 1618600\n",
            "Current start time (in ms): 1619080\n",
            "Current start time (in ms): 1627420\n",
            "Current start time (in ms): 1631040\n",
            "Current start time (in ms): 1638100\n",
            "Current start time (in ms): 1643360\n",
            "Current start time (in ms): 1647180\n",
            "Current start time (in ms): 1651680\n",
            "Current start time (in ms): 1656560\n",
            "Current start time (in ms): 1657480\n",
            "Current start time (in ms): 1659560\n",
            "Current start time (in ms): 1666980\n",
            "Current start time (in ms): 1668520\n",
            "Current start time (in ms): 1679240\n",
            "Current start time (in ms): 1682720\n",
            "Current start time (in ms): 1682880\n",
            "Current start time (in ms): 1684540\n",
            "Current start time (in ms): 1688860\n",
            "Current start time (in ms): 1691660\n"
          ]
        }
      ],
      "source": [
        "with open(f\"{os.path.splitext(audio_path)[0]}.txt\", \"w\", encoding=\"utf-8-sig\") as f:\n",
        "    get_speaker_aware_transcript_time(ssm, f)\n",
        "\n",
        "with open(f\"{os.path.splitext(audio_path)[0]}.srt\", \"w\", encoding=\"utf-8-sig\") as srt:\n",
        "    write_srt(ssm, srt)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "eCmjcOc9yEtQ",
        "jbsUt3SwyhjD"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}